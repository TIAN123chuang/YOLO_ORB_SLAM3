class Model(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  model : __torch__.torch.nn.modules.container.___torch_mangle_267.Sequential
  def forward(self: __torch__.models.yolo.Model,
    x: Tensor) -> Tuple[Tensor, List[Tensor]]:
    _0 = getattr(self.model, "24")
    _1 = getattr(self.model, "23")
    _2 = getattr(self.model, "22")
    _3 = getattr(self.model, "21")
    _4 = getattr(self.model, "20")
    _5 = getattr(self.model, "19")
    _6 = getattr(self.model, "18")
    _7 = getattr(self.model, "17")
    _8 = getattr(self.model, "16")
    _9 = getattr(self.model, "15")
    _10 = getattr(self.model, "14")
    _11 = getattr(self.model, "13")
    _12 = getattr(self.model, "12")
    _13 = getattr(self.model, "11")
    _14 = getattr(self.model, "10")
    _15 = getattr(self.model, "9")
    _16 = getattr(self.model, "8")
    _17 = getattr(self.model, "7")
    _18 = getattr(self.model, "6")
    _19 = getattr(self.model, "5")
    _20 = getattr(self.model, "4")
    _21 = getattr(self.model, "3")
    _22 = getattr(self.model, "2")
    _23 = getattr(self.model, "1")
    _24 = (getattr(self.model, "0")).forward(x, )
    _25 = (_22).forward((_23).forward(_24, ), )
    _26 = (_20).forward((_21).forward(_25, ), )
    _27 = (_18).forward((_19).forward(_26, ), )
    _28 = (_16).forward((_17).forward(_27, ), )
    _29 = (_14).forward((_15).forward(_28, ), )
    _30 = (_12).forward((_13).forward(_29, ), _27, )
    _31 = (_10).forward((_11).forward(_30, ), )
    _32 = (_8).forward((_9).forward(_31, ), _26, )
    _33 = (_7).forward(_32, )
    _34 = (_5).forward((_6).forward(_33, ), _31, )
    _35 = (_4).forward(_34, )
    _36 = (_2).forward((_3).forward(_35, ), _29, )
    _37 = (_0).forward(_33, _35, (_1).forward(_36, ), )
    _38, _39, _40, _41, = _37
    return (_41, [_38, _39, _40])
class Detect(Module):
  __parameters__ = []
  __buffers__ = ["anchors", "anchor_grid", ]
  anchors : Tensor
  anchor_grid : Tensor
  training : bool
  m : __torch__.torch.nn.modules.container.___torch_mangle_266.ModuleList
  def forward(self: __torch__.models.yolo.Detect,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    _42 = getattr(self.m, "2")
    _43 = getattr(self.m, "1")
    _44 = self.anchor_grid
    _45 = (getattr(self.m, "0")).forward(argument_1, )
    bs = ops.prim.NumToTensor(torch.size(_45, 0))
    _46 = int(bs)
    _47 = int(bs)
    ny = ops.prim.NumToTensor(torch.size(_45, 2))
    _48 = int(ny)
    nx = ops.prim.NumToTensor(torch.size(_45, 3))
    _49 = torch.view(_45, [_47, 3, 85, _48, int(nx)])
    _50 = torch.contiguous(torch.permute(_49, [0, 1, 3, 4, 2]), memory_format=0)
    y = torch.sigmoid(_50)
    _51 = torch.mul(torch.slice(y, 4, 0, 2, 1), CONSTANTS.c1)
    _52 = torch.sub(_51, CONSTANTS.c2, alpha=1)
    _53 = torch.to(CONSTANTS.c3, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)
    _54 = torch.mul(torch.add(_52, _53, alpha=1), torch.select(CONSTANTS.c4, 0, 0))
    _55 = torch.slice(y, 4, 0, 2, 1)
    _56 = torch.expand(torch.view(_54, [3, 48, 80, 2]), [1, 3, 48, 80, 2], implicit=True)
    _57 = torch.copy_(_55, _56, False)
    _58 = torch.mul(torch.slice(y, 4, 2, 4, 1), CONSTANTS.c5)
    _59 = torch.mul(torch.pow(_58, 2), torch.select(_44, 0, 0))
    _60 = torch.slice(y, 4, 2, 4, 1)
    _61 = torch.expand(torch.view(_59, [3, 48, 80, 2]), [1, 3, 48, 80, 2], implicit=True)
    _62 = torch.copy_(_60, _61, False)
    _63 = torch.view(y, [_46, -1, 85])
    _64 = (_43).forward(argument_2, )
    bs0 = ops.prim.NumToTensor(torch.size(_64, 0))
    _65 = int(bs0)
    _66 = int(bs0)
    ny0 = ops.prim.NumToTensor(torch.size(_64, 2))
    _67 = int(ny0)
    nx0 = ops.prim.NumToTensor(torch.size(_64, 3))
    _68 = torch.view(_64, [_66, 3, 85, _67, int(nx0)])
    _69 = torch.contiguous(torch.permute(_68, [0, 1, 3, 4, 2]), memory_format=0)
    y0 = torch.sigmoid(_69)
    _70 = torch.mul(torch.slice(y0, 4, 0, 2, 1), CONSTANTS.c1)
    _71 = torch.sub(_70, CONSTANTS.c2, alpha=1)
    _72 = torch.to(CONSTANTS.c6, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)
    _73 = torch.mul(torch.add(_71, _72, alpha=1), torch.select(CONSTANTS.c4, 0, 1))
    _74 = torch.slice(y0, 4, 0, 2, 1)
    _75 = torch.expand(torch.view(_73, [3, 24, 40, 2]), [1, 3, 24, 40, 2], implicit=True)
    _76 = torch.copy_(_74, _75, False)
    _77 = torch.mul(torch.slice(y0, 4, 2, 4, 1), CONSTANTS.c5)
    _78 = torch.mul(torch.pow(_77, 2), torch.select(_44, 0, 1))
    _79 = torch.slice(y0, 4, 2, 4, 1)
    _80 = torch.expand(torch.view(_78, [3, 24, 40, 2]), [1, 3, 24, 40, 2], implicit=True)
    _81 = torch.copy_(_79, _80, False)
    _82 = torch.view(y0, [_65, -1, 85])
    _83 = (_42).forward(argument_3, )
    bs1 = ops.prim.NumToTensor(torch.size(_83, 0))
    _84 = int(bs1)
    _85 = int(bs1)
    ny1 = ops.prim.NumToTensor(torch.size(_83, 2))
    _86 = int(ny1)
    nx1 = ops.prim.NumToTensor(torch.size(_83, 3))
    _87 = torch.view(_83, [_85, 3, 85, _86, int(nx1)])
    _88 = torch.contiguous(torch.permute(_87, [0, 1, 3, 4, 2]), memory_format=0)
    y1 = torch.sigmoid(_88)
    _89 = torch.mul(torch.slice(y1, 4, 0, 2, 1), CONSTANTS.c1)
    _90 = torch.sub(_89, CONSTANTS.c2, alpha=1)
    _91 = torch.to(CONSTANTS.c7, dtype=6, layout=0, device=torch.device("cpu"), pin_memory=False, non_blocking=False, copy=False, memory_format=None)
    _92 = torch.mul(torch.add(_90, _91, alpha=1), torch.select(CONSTANTS.c4, 0, 2))
    _93 = torch.slice(y1, 4, 0, 2, 1)
    _94 = torch.expand(torch.view(_92, [3, 12, 20, 2]), [1, 3, 12, 20, 2], implicit=True)
    _95 = torch.copy_(_93, _94, False)
    _96 = torch.mul(torch.slice(y1, 4, 2, 4, 1), CONSTANTS.c5)
    _97 = torch.mul(torch.pow(_96, 2), torch.select(_44, 0, 2))
    _98 = torch.slice(y1, 4, 2, 4, 1)
    _99 = torch.expand(torch.view(_97, [3, 12, 20, 2]), [1, 3, 12, 20, 2], implicit=True)
    _100 = torch.copy_(_98, _99, False)
    _101 = [_63, _82, torch.view(y1, [_84, -1, 85])]
    return (_50, _69, _88, torch.cat(_101, 1))
